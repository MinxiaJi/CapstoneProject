{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capstone\n",
    "## Machine Learning Engineer Nanodegree\n",
    "## Build Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data and split train/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load data\n",
    "import pandas as pd\n",
    "data = pd.read_csv('C:/Users/youch/Desktop/september_jmx/credit_default_pre.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Splitting data to train and test\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train,x_test,y_train,y_test = train_test_split(data.drop('default payment next month',axis=1),\n",
    "                                                data['default payment next month'],test_size = 0.2,random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose classifiers based on 3 datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Supervised Learning Models\n",
    "**The following supervised learning models are currently available in** [`scikit-learn`](http://scikit-learn.org/stable/supervised_learning.html) **that I choose from:**\n",
    "- GradientBoosting Classifier\n",
    "- RandomForest Classifier\n",
    "- Logistic Regression\n",
    "\n",
    "###  Three datasets\n",
    "- Imbalanced dataset\n",
    "- Oversampling\n",
    "- Undersampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import necessary modules\n",
    "from sklearn.metrics import fbeta_score\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# evaluate classifiers based on Fbetascore and \n",
    "# FNR:false negative rate\n",
    "def evaluation_function(classifier,x_train,y_train,x_test,y_test):\n",
    "    \"\"\"Input classifier and train/test data, return Fbetascore and False negative rate\"\"\"\n",
    "    classifier.fit(x_train,y_train)\n",
    "    predicted_labels = classifier.predict(x_test)\n",
    "    TP,TN,FP,FN = 0,0,0,0\n",
    "    for i in range(len(predicted_labels)):\n",
    "        if predicted_labels[i] == y_test.values[i] == 1:\n",
    "            TP += 1\n",
    "        elif predicted_labels[i] == y_test.values[i] == 0:\n",
    "            TN += 1\n",
    "        elif predicted_labels[i] ==1 and y_test.values[i] == 0:\n",
    "            FP += 1\n",
    "        else:\n",
    "            FN += 1\n",
    "    FNR = float(FP)/float(TP+FP)\n",
    "    fbeta = fbeta_score(y_test, predicted_labels, beta = 0.5)\n",
    "    print 'For the learner',classifier,'the false negative rate is {} and the f0.5beta score is {}'.format(FNR,fbeta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Choose classifier for imbalanced dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For the learner GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
      "              learning_rate=0.1, loss='deviance', max_depth=3,\n",
      "              max_features=None, max_leaf_nodes=None,\n",
      "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "              min_samples_leaf=1, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
      "              warm_start=False) the false negative rate is 0.337110481586 and the f0.5beta score is 0.565627266135\n",
      "For the learner RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False) the false negative rate is 0.395802098951 and the f0.5beta score is 0.506154232605\n",
      "For the learner LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False) the false negative rate is 0.0 and the f0.5beta score is 0.00757002271007\n"
     ]
    }
   ],
   "source": [
    "# evaluate 3 classifiers to see which classifier performed best in Fbetascore and False negative rate\n",
    "evaluation_function(GradientBoostingClassifier(), x_train, y_train, x_test, y_test)\n",
    "evaluation_function(RandomForestClassifier(), x_train, y_train, x_test, y_test)\n",
    "evaluation_function(LogisticRegression(), x_train, y_train, x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the results above, we should choose GradientBoostingClassifier as our learner since the f0.5beta score is the highest among 3 classifiers, and the false negative rate is also relatively low. However the false negative rate, 0.34, is still very high. It means out of 100 customers who are going to default, we will misclassified approximately 34 customers to be not going to default, which is not good. That might because the data imbalancement issue. We need to figure our it later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Choose classifier for undersampling dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# construct undersampling dataset\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "def undersampling(data,major_label,minor_label,label_name):\n",
    "    major_index = data[data[label_name] == major_label].index\n",
    "    len_minor = len(data[data[label_name] == minor_label].index)\n",
    "    random.seed(42)\n",
    "    labels = random.sample(major_index,len_minor)\n",
    "    reduced_major = pd.DataFrame()\n",
    "    for i in labels:\n",
    "        temp = data.loc[[i]]\n",
    "        reduced_major = pd.concat([reduced_major,temp])\n",
    "    under_df = pd.concat([reduced_major,  data[data[label_name] == minor_label]])\n",
    "    return under_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xy_train = pd.concat([x_train,y_train],axis = 1)\n",
    "under_df = undersampling(xy_train,0,1,'default payment next month')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ux_train = under_df.drop('default payment next month',axis=1)\n",
    "uy_train = under_df['default payment next month']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For the learner GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
      "              learning_rate=0.1, loss='deviance', max_depth=3,\n",
      "              max_features=None, max_leaf_nodes=None,\n",
      "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "              min_samples_leaf=1, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
      "              warm_start=False) the false negative rate is 0.542122538293 and the f0.5beta score is 0.485217391304\n",
      "For the learner RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False) the false negative rate is 0.575707154742 and the f0.5beta score is 0.448680351906\n",
      "For the learner LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False) the false negative rate is 0.574297188755 and the f0.5beta score is 0.456847322487\n"
     ]
    }
   ],
   "source": [
    "# evaluate 3 classifiers to see which classifier performed best in Fbetascore and False negative rate\n",
    "evaluation_function(GradientBoostingClassifier(), ux_train, uy_train, x_test, y_test)\n",
    "evaluation_function(RandomForestClassifier(), ux_train, uy_train, x_test, y_test)\n",
    "evaluation_function(LogisticRegression(), ux_train, uy_train, x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose GBC as the classifier for undersampling dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Choose classifier for oversampling dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset shape Counter({0: 18677, 1: 5323})\n",
      "Resampled dataset shape Counter({0: 18677, 1: 18677})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from sklearn.datasets import make_classification\n",
    "from imblearn.over_sampling import SMOTE \n",
    "\n",
    "X, y = make_classification(n_classes=2, class_sep=2,\n",
    "weights=[0.1, 0.9], n_informative=3, n_redundant=1, flip_y=0,\n",
    "n_features=20, n_clusters_per_class=1, n_samples=1000, random_state=10)\n",
    "\n",
    "print('Original dataset shape {}'.format(Counter(y_train)))\n",
    "sm = SMOTE(random_state=42)\n",
    "x_res, y_res = sm.fit_sample(x_train,y_train)\n",
    "print('Resampled dataset shape {}'.format(Counter(y_res)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# traning/testing splitting\n",
    "ox_train,ox_test,oy_train,oy_test = train_test_split(x_res,y_res,test_size = 0.2,random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# because oversampling dataset consists of array, we need to modify evaluation function a little bit\n",
    "def evaluation(classifier,x_train,y_train,x_test,y_test):\n",
    "    \"\"\"Input classifier and train/test data, return Fbetascore and False negative rate\"\"\"\n",
    "    classifier.fit(x_train,y_train)\n",
    "    predicted_labels = classifier.predict(x_test)\n",
    "    TP,TN,FP,FN = 0,0,0,0\n",
    "    for i in range(len(predicted_labels)):\n",
    "        if predicted_labels[i] == y_test[i] == 1:\n",
    "            TP += 1\n",
    "        elif predicted_labels[i] == y_test[i] == 0:\n",
    "            TN += 1\n",
    "        elif predicted_labels[i] ==1 and y_test[i] == 0:\n",
    "            FP += 1\n",
    "        else:\n",
    "            FN += 1\n",
    "    FNR = float(FP)/float(TP+FP)\n",
    "    fbeta = fbeta_score(y_test, predicted_labels, beta = 0.5)\n",
    "    print 'For the learner',classifier,'the false negative rate is {} and the f0.5beta score is {}'.format(FNR,fbeta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For the learner GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
      "              learning_rate=0.1, loss='deviance', max_depth=3,\n",
      "              max_features=None, max_leaf_nodes=None,\n",
      "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "              min_samples_leaf=1, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
      "              warm_start=False) the false negative rate is 0.392307692308 and the f0.5beta score is 0.558247526751\n",
      "For the learner RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False) the false negative rate is 0.433939393939 and the f0.5beta score is 0.506178192066\n",
      "For the learner LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False) the false negative rate is 0.561027837259 and the f0.5beta score is 0.466704610131\n"
     ]
    }
   ],
   "source": [
    "# evaluate 3 classifiers to see which classifier performed best in Fbetascore and False negative rate\n",
    "evaluation_function(GradientBoostingClassifier(), x_res, y_res, x_test, y_test)\n",
    "evaluation_function(RandomForestClassifier(), x_res, y_res, x_test, y_test)\n",
    "evaluation_function(LogisticRegression(), x_res, y_res, x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose GBC for oversampling dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Supervised Learning Models\n",
    "- GradientBoosting Classifier for imbalanced dataset and undersampling dataset\n",
    "- RandomForest Classifier for oversampling dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# initialize GBC paramters wants to try\n",
    "params_gboost = {'learning_rate':[0.05,0.1,0.2],\n",
    "                 # 0.5%-1% of data\n",
    "                'min_samples_split':[300],\n",
    "                 'n_estimators':[100],\n",
    "                'max_depth':[15],\n",
    "                 # help to prevent overfitting\n",
    "                'min_samples_leaf':[30]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def optimize_model(classifier,params2try,x_train,y_train,x_test,y_test):\n",
    "    \"\"\"Employ grid search to find the best parameter for the classifier. Grid search is based on Fbetascore\"\"\"\n",
    "    \n",
    "    clf = classifier(random_state=42)\n",
    "    scorer = make_scorer(fbeta_score, beta=0.5)\n",
    "    \n",
    "    grid_search = GridSearchCV(clf,param_grid = params2try,scoring = scorer)\n",
    "    #train data on training set using grid search and find the best model with certain paramters\n",
    "    grid_train = grid_search.fit(x_train,y_train)\n",
    "    optimized_clf = grid_train.best_estimator_\n",
    "    \n",
    "    # predcition on testing data\n",
    "    unoptimized_preds = (clf.fit(x_train,y_train)).predict(x_test)\n",
    "    optimized_preds = optimized_clf.predict(x_test)\n",
    "    \n",
    "\n",
    "    print (\"Unoptimized model\\n------\")\n",
    "    print (\"F-score on testing data: {:.4f}\".format(fbeta_score(y_test, unoptimized_preds, beta = 0.5)))\n",
    "    print (\"\\nOptimized Model\\n------\")\n",
    "    print (\"Final F-score on the testing data: {:.4f}\".format(fbeta_score(y_test, optimized_preds, beta = 0.5)))\n",
    "    print optimized_clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Try with imbalanced data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unoptimized model\n",
      "------\n",
      "F-score on testing data: 0.5656\n",
      "\n",
      "Optimized Model\n",
      "------\n",
      "Final F-score on the testing data: 0.5823\n",
      "GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
      "              learning_rate=0.05, loss='deviance', max_depth=15,\n",
      "              max_features=None, max_leaf_nodes=None,\n",
      "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "              min_samples_leaf=30, min_samples_split=300,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "              presort='auto', random_state=42, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "# get Fbetascore and best classifier\n",
    "optimize_model(GradientBoostingClassifier,params_gboost,x_train,y_train,x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# initialize best classifier\n",
    "optimized_GBC = GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
    "              learning_rate=0.05, loss='deviance', max_depth=15,\n",
    "              max_features=None, max_leaf_nodes=None,\n",
    "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "              min_samples_leaf=30, min_samples_split=300,\n",
    "              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
    "              presort='auto', random_state=42, subsample=1.0, verbose=0,\n",
    "              warm_start=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For the learner GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
      "              learning_rate=0.05, loss='deviance', max_depth=15,\n",
      "              max_features=None, max_leaf_nodes=None,\n",
      "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "              min_samples_leaf=30, min_samples_split=300,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "              presort='auto', random_state=42, subsample=1.0, verbose=0,\n",
      "              warm_start=False) the false negative rate is 0.324657534247 and the f0.5beta score is 0.582329317269\n"
     ]
    }
   ],
   "source": [
    "evaluation_function(optimized_GBC, x_train, y_train, x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compared to classifier, GBC, with default setting, the GBC after parameter tunning performed slightly better both in false negative rate and in f0.5beta score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Try with balanced data: undersampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unoptimized model\n",
      "------\n",
      "F-score on testing data: 0.4973\n",
      "\n",
      "Optimized Model\n",
      "------\n",
      "Final F-score on the testing data: 0.5140\n",
      "GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
      "              learning_rate=0.05, loss='deviance', max_depth=15,\n",
      "              max_features=None, max_leaf_nodes=None,\n",
      "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "              min_samples_leaf=30, min_samples_split=300,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "              presort='auto', random_state=42, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "optimize_model(GradientBoostingClassifier,params_gboost,ux_train,\n",
    "               uy_train,x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "opt_for_under = GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
    "              learning_rate=0.05, loss='deviance', max_depth=15,\n",
    "              max_features=None, max_leaf_nodes=None,\n",
    "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "              min_samples_leaf=30, min_samples_split=300,\n",
    "              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
    "              presort='auto', random_state=42, subsample=1.0, verbose=0,\n",
    "              warm_start=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notice:\n",
    "Knowing that if we want to evaluate wether using a undersampling dataset is good or not, we should control: test data. So we should check the performance on x_test we've already splitted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For the learner GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
      "              learning_rate=0.05, loss='deviance', max_depth=15,\n",
      "              max_features=None, max_leaf_nodes=None,\n",
      "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "              min_samples_leaf=30, min_samples_split=300,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "              presort='auto', random_state=42, subsample=1.0, verbose=0,\n",
      "              warm_start=False) the false negative rate is 0.520485584219 and the f0.5beta score is 0.514044029932\n"
     ]
    }
   ],
   "source": [
    "evaluation_function(opt_for_under, ux_train,\n",
    "               uy_train, x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model feed with undersampling data performed worse! The false negative rate went up a lot and f0.5beta score went down a lot! That's might due to that we have less data available. Let's try oversampling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### try with balanced data: oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unoptimized model\n",
      "------\n",
      "F-score on testing data: 0.5582\n",
      "\n",
      "Optimized Model\n",
      "------\n",
      "Final F-score on the testing data: 0.5465\n",
      "GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
      "              learning_rate=0.05, loss='deviance', max_depth=15,\n",
      "              max_features=None, max_leaf_nodes=None,\n",
      "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "              min_samples_leaf=30, min_samples_split=300,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "              presort='auto', random_state=42, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "optimize_model(GradientBoostingClassifier,params_gboost,x_res,y_res,x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "opt_for_over = GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
    "              learning_rate=0.05, loss='deviance', max_depth=15,\n",
    "              max_features=None, max_leaf_nodes=None,\n",
    "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "              min_samples_leaf=30, min_samples_split=300,\n",
    "              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
    "              presort='auto', random_state=42, subsample=1.0, verbose=0,\n",
    "              warm_start=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For the learner GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
      "              learning_rate=0.05, loss='deviance', max_depth=15,\n",
      "              max_features=None, max_leaf_nodes=None,\n",
      "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "              min_samples_leaf=30, min_samples_split=300,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "              presort='auto', random_state=42, subsample=1.0, verbose=0,\n",
      "              warm_start=False) the false negative rate is 0.400452488688 and the f0.5beta score is 0.546504433904\n"
     ]
    }
   ],
   "source": [
    "evaluation_function(opt_for_over, x_res,y_res, x_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
